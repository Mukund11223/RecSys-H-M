{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f64fada",
   "metadata": {},
   "source": [
    "# Baseline Popularity Model for Recommendation System\n",
    "This notebook implements several baseline recommendation models for the H&M Personalized Fashion Recommendations challenge. It loads the data, explores user history, and builds three types of recommenders:\n",
    "- **Global Popularity**: Recommends the most popular items overall.\n",
    "- **Personal Popularity (Recency-Weighted)**: Recommends items based on each user's purchase history, with more recent purchases weighted higher.\n",
    "- **Category-Affinity Baseline**: Recommends popular items within a user's favorite product categories.\n",
    "\n",
    "The notebook also generates submission files for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a428e27c",
   "metadata": {},
   "source": [
    "## Importing Libraries and Loading Data\n",
    "We start by importing essential libraries for data manipulation and visualization. The main datasets (`articles.csv`, `customers.csv`, `transactions.parquet`, and `sample_submission.csv`) are loaded for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c06283b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfafbf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv(\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bc4026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_csv('articles.csv')\n",
    "customers = pd.read_csv('customers.csv')\n",
    "transactions_parquet = pd.read_parquet('transactions.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282a45e2",
   "metadata": {},
   "source": [
    "## Date Conversion and Transaction Preparation\n",
    "To efficiently work with transaction dates, we define a memoized date conversion function. The transaction data is then processed to ensure date columns are in the correct format for time-based analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b39ad98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def convert_to_date(s):\n",
    "    \"\"\"\n",
    "    Memoization technique - very fast conversion to pure python dates\n",
    "    \"\"\"\n",
    "    dates = {date:datetime.datetime.strptime(date,'%Y-%m-%d') for date in s.unique()}\n",
    "    return s.map(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd8de367",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_parquet[\"t_dat\"] = convert_to_date(transactions_parquet[\"t_dat\"])\n",
    "transactions_parquet[\"t_dat\"] = pd.to_datetime(transactions_parquet[\"t_dat\"])\n",
    "transactions = transactions_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9da3695",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = transactions\n",
    "arts = articles\n",
    "cust = customers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727287fc",
   "metadata": {},
   "source": [
    "## User History Extraction\n",
    "We extract each user's purchase history by grouping transactions by customer and collecting their purchased article IDs in order. This will be useful for personalized recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e9bd47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_hist = (\n",
    "    tx.sort_values(\"t_dat\")\n",
    "      .groupby(\"customer_id\")[\"article_id\"]\n",
    "      .apply(list)\n",
    "      .to_dict()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c20fa664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad to 10 digits (e.g., 706016001 -> \"0706016001\")\n",
    "fmt = lambda a: str(a).zfill(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd515db7",
   "metadata": {},
   "source": [
    "## Global Popularity Model\n",
    "The global popularity model recommends the top-N most purchased items to every user, regardless of their individual history. This serves as a simple but strong baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b997e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute global top-N\n",
    "N = 12\n",
    "global_topN = tx['article_id'].value_counts().index[:N].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ebe3f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_global(uid):\n",
    "    return global_topN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb1f43f",
   "metadata": {},
   "source": [
    "## Personal Popularity (Recency-Weighted) Model\n",
    "This model recommends items based on each user's own purchase history, giving higher weight to more recent purchases. It uses exponential decay to prioritize recent activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236d7002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def build_user_profiles(tx: pd.DataFrame, decay: float = 0.9, N: int = 12) -> dict:\n",
    "    # 1. Compute weights\n",
    "    now = tx['t_dat'].max()\n",
    "    df = tx.copy()\n",
    "    df['days_ago'] = (now - df['t_dat']).dt.days\n",
    "    df['weight'] = decay ** (df['days_ago'] / 7)\n",
    "\n",
    "    # 2. Aggregate weights per (user, item)\n",
    "    user_item = df.groupby(['customer_id', 'article_id'], as_index=False)['weight'].sum()\n",
    "\n",
    "    # 3. Build profiles with tqdm\n",
    "    user_profiles = {}\n",
    "    total_users = user_item['customer_id'].nunique()\n",
    "    for uid, grp in tqdm(user_item.groupby('customer_id'), \n",
    "                        desc='Building user profiles', \n",
    "                        total=total_users, \n",
    "                        unit='user'):\n",
    "        top_articles = grp.nlargest(N, 'weight')['article_id'].astype(str).tolist()\n",
    "        user_profiles[uid] = top_articles\n",
    "\n",
    "    return user_profiles\n",
    "\n",
    "user_profiles = build_user_profiles(tx, decay=0.9, N=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccc48982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_personal(uid):\n",
    "    return user_profiles.get(uid, global_topN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e498537",
   "metadata": {},
   "source": [
    "## Category-Affinity Baseline Model\n",
    "This model recommends the most popular items within each user's top-2 most purchased product categories. It combines user history with global category trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c9d08de",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_by_cat = (\n",
    "    tx\n",
    "    .merge(arts[['article_id','product_type_name']], on='article_id')\n",
    "    .groupby('product_type_name')['article_id']\n",
    "    .value_counts()\n",
    "    .groupby(level=0)\n",
    "    .head(20)\n",
    "    .reset_index(name='count')\n",
    "    .groupby('product_type_name')['article_id']\n",
    "    .apply(list)\n",
    "    .to_dict()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05fc4c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "id_to_cat = dict(zip(\n",
    "  arts['article_id'],\n",
    "  arts['product_type_name']\n",
    "))\n",
    "\n",
    "def recommend_by_category(uid, N=12):\n",
    "    hist = user_hist.get(uid, [])\n",
    "    # map each article to its category via dict lookup\n",
    "    cats = Counter(id_to_cat.get(a, None) for a in hist).most_common(2)\n",
    "\n",
    "    recs = []\n",
    "    for cat, _ in cats:\n",
    "        recs.extend(pop_by_cat.get(cat, []))\n",
    "    return recs[:N] if recs else global_topN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ec0959",
   "metadata": {},
   "source": [
    "## Generating Submission Files\n",
    "For each model, we generate a submission file in the required format. Each file contains customer IDs and their recommended articles, padded to 10 digits, ready for submission to the kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d30de51",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"global\"   : recommend_global,\n",
    "    \"personal\" : recommend_personal,\n",
    "    \"category\" : recommend_by_category\n",
    "}\n",
    "\n",
    "# pad any id to 10 digits\n",
    "pad10 = lambda a: str(a).zfill(10)\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "for name, func in models.items():\n",
    "    sub = sample[['customer_id']].copy()\n",
    "    customer_ids = sub['customer_id'].tolist()\n",
    "\n",
    "    preds = []\n",
    "    for uid in tqdm(customer_ids, desc=f\"Model: {name}\", unit=\"cust\"):\n",
    "        preds.append(\" \".join(pad10(x) for x in func(uid)))  # <- pad here\n",
    "\n",
    "    sub['prediction'] = preds\n",
    "    fname = f\"submission_{name}.csv\"\n",
    "    sub.to_csv(fname, index=False)\n",
    "    print(f\"✏️  Wrote {fname} ({len(sub)} rows)\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
